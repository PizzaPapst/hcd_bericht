\chapter{Automatische Analyse}
Die automatisierte Analyse der Startseite sowie der Unterseiten foehr.de/barrierefrei und foehr.de/gezeiten erfolgte im Rahmen eines digitalen Gruppentreffens am 21. Dezember 2025 von ca. 10:00–13:30 Uhr (inklusive Vor- und Nachbesprechung). Die drei Gruppenmitglieder testeten alle Seiten jeweils mit einem unterschiedlichen Analysetool und Geräten. Als Analysetools wurden die weitverbreiteten Tools WAVE \autocite{noauthor_wave_nodate}, Axe DevTools \autocite{noauthor_axe_nodate} und Google Lighthouse \autocite{noauthor_lighthouse_nodate} verwendet. Die folgende Auflistung zeigt die Testumgebung der Einzelnen Teilnehmer:

\begin{itemize}
    \item Axe DevTools (Chrome, MacBook, 16 Zoll Bildschirm)
    \item WAVE (Chrome, Laptop, Windows, 27 Zoll Bildschirm)
    \item Lighthouse (Chrome, MacBook, 27 Zoll Bildschirm)
\end{itemize}

In dem nachfolgenden Unterkapitel werden die Ergebnisse werden die zentralen Analyseergebnisse getrennt nach Analysetool sowie eine Bewertung der Ergebnisse in Auszügen dargestellt. Fehler, die allgemeine Komponenten (wie Header und Footer) oder Styleguide-Vorgaben betreffen (z. B. unzureichender Text-/Farbkontrast), wurden pro Analyse-Tool nur einmalig für die Startseite dokumentiert, selbst wenn sie vom Tool mehrfach bemängelt wurden. Die vollständigen Testtabellen sind in \autoref{app:autoAxe}, \autoref{app:autoWAVE} und \autoref{app:autoLighthouse} zu finden.

\section{Axe DevTools}
Axe ließ sich als Browsererweiterung einbinden und über die DevTools aufrufen. Für die Tests in diesem Modul wurde eine 7-tägige Testversion der Vollversion genutzt, um auf alle Features zugreifen zu können; vollautomatische Tests sind jedoch auch im Free-Plan verfügbar. Axe unterteilt den Prüfprozess in drei Abschnitte: eine automatische Überprüfung, halbautomatische Tests und manuelle Tests. Im Rahmen dieser Aufgabe wurde lediglich das vollautomatische Testverfahren der Axe DevTools angewendet.

Die Untersuchung ergab auf sämtlichen getesteten Seiten Verstöße gegen die Barrierefreiheit. Im Detail wurden auf der Startseite zwei, auf der Barrierefrei-Seite drei und auf der Gezeiten-Seite zwei Kriterien nicht erfüllt. Die identifizierten Barrieren betreffen die Prüfschritte 9.1.1.1b (Alternativtexte für Grafiken und Objekte), 9.1.4.3 (Kontraste von Texten), 9.1.3.1h (Beschriftung von Formularelementen programmatisch ermittelbar) sowie 9.4.1.2 (Name, Rolle, Wert). Eine detaillierte Aufstellung der Ergebnisse inklusive weiterführender Hinweise sind in \autoref{tab:testergebnisse-startseite-axe}, \autoref{tab:testergebnisse-barrierefrei-axe} und \autoref{tab:testergebnisse-gezeiten-axe} zu finden.

Der automatisierte Test fand zuverlässig Barrieren, es traten dabei keine falsch-positiven Ergebnisse ("`Findings"') auf. Die rein automatischen Tests lieferten quantitativ eher wenige Resultate, da Axe primär den Ansatz der halbautomatischen Tests (IGTs) verfolgt. Bei diesen IGTs handelt es sich um automatische Tests, die eine Überprüfung durch einen Tester erfordern ("`Human-in-the-Loop"'). Dies führt dazu, dass der vollautomatische Test eher konservativ agiert und nur jene Fehler meldet, die eindeutig programmseitig erkennbar sind.

Hinsichtlich der Bedienung und Erklärungen erwies sich Axe als sehr nützliches Werkzeug. Barrieren wurden gut aufbereitet und ließen sich auf der Seite hervorheben, zudem lieferte das Tool Erklärungen sowie Verbesserungsvorschläge. Lediglich die Möglichkeit, falsch-positive Ergebnisse manuell zu entfernen, fehlte.

\section{Wave}
Die Untersuchung ergab auf sämtlichen getesteten Seiten Verstöße gegen die Barrierefreiheit. Im Detail wurden auf der Startseite vier, auf der Barrierefrei-Seite zwei und auf der Gezeiten-Seite drei Kriterien nicht erfüllt. Die identifizierten Barrieren betreffen die Prüfschritte 9.1.1.1b (Alternativtexte für Grafiken und Objekte), 9.1.4.3 (Kontraste von Texten), 9.2.4.4 (Aussagekräftige Linktexte), 9.2.4.6 (Aussagekräftige Überschriften und Beschriftungen) sowie 9.3.3.2 (Beschriftungen von Formularelementen vorhanden). Eine detaillierte Aufstellung der Ergebnisse inklusive weiterführender Hinweise sind in \autoref{tab:testergebnisse-startseite-wave}, \autoref{tab:testergebnisse-barrierefrei-wave} und \autoref{tab:testergebnisse-gezeiten-wave} zu finden.

Durch die Browser-Extension ließ sich WAVE schnell und unkompliziert in Chrome nutzen. Ein Vorteil beim Testing war, dass die Ergebnisse direkt auf der Webseite durch verschiedene Icons visualisiert wurden. Fehler, Warnungen und Hinweise wurden unmittelbar im Layout markiert, was die Analyse erleichtert. Teilweise erwies sich die Sprungmarke zum jeweiligen Fehler jedoch als unübersichtlich, sodass die betroffene Stelle nicht immer eindeutig identifiziert werden konnte. In diesen Fällen war es notwendig, zusätzlich die Chrome DevTools zu nutzen, um die genaue Position im Code zu finden. Sehr hilfreich war allerdings, dass zu jedem erkannten Problem verständliche Erklärungen, konkrete Lösungsvorschläge sowie ein direkter Bezug zu den entsprechenden WCAG-Erfolgskriterien bereitgestellt werden.

Wie bei vielen automatisierten Tools sind auch bei WAVE die gefundenen Fehler und Alerts häufig kontextabhängig und erfordern Fachwissen für eine korrekte Bewertung. Im Vergleich zu Axe und Lighthouse fand WAVE im durchgeführten Test jedoch die meisten Fehler und Alerts.


\section{Google Lighthouse}
Die Untersuchung ergab auf sämtlichen getesteten Seiten Verstöße gegen die Barrierefreiheit. Im Detail wurden auf der Startseite drei, auf der Barrierefrei-Seite drei und auf der Gezeiten-Seite zwei Kriterien nicht erfüllt. Die identifizierten Barrieren betreffen die Prüfschritte 9.1.1.1b (Alternativtexte für Grafiken und Objekte), 9.1.3.1a (HTML-Strukturelemente für Überschriften), 9.1.3.1h (Beschriftung von Formularelementen programmatisch ermittelbar), 9.1.4.3 (Kontraste von Texten) sowie 9.4.1.2 (Name, Rolle, Wert). Eine detaillierte Aufstellung der Ergebnisse inklusive weiterführender Hinweise sind in \autoref{tab:testergebnisse-startseite-lighthouse}, \autoref{tab:testergebnisse-barrierefrei-lighthouse} und \autoref{tab:testergebnisse-gezeiten-lighthouse} zu finden.

Die Analyse mit Google Lighthouse erfolgte direkt über die Chrome DevTools und ließ sich ohne zusätzliche Erweiterungen durchführen. Lighthouse basiert für die Barrierefreiheitsprüfung ebenfalls auf der Prüf-Engine axe-core und liefert daher überwiegend vergleichbare Ergebnisse zu axe. Der Fokus liegt auf vollautomatisch erkennbaren Problemen, wodurch die Anzahl der gefundenen Barrieren insgesamt eher begrenzt ist.

Die Ergebnisse werden in Form eines strukturierten Berichts mit einer numerischen Bewertung sowie einer Liste konkreter Prüfungen dargestellt. Positiv hervorzuheben ist, dass Lighthouse zu jedem gefundenen Problem eine kurze Beschreibung, Hinweise zur Behebung und einen Bezug zu den entsprechenden WCAG-Erfolgskriterien bereitstellt. Dadurch eignet sich das Tool gut für eine erste Einschätzung des Barrierefreiheitsstatus einer Seite.

Im Gegensatz zu WAVE werden die gefundenen Probleme jedoch nicht direkt im Seitenlayout visualisiert, sondern ausschließlich im Bericht angezeigt. Um die betroffenen Stellen im Code eindeutig zu identifizieren, ist daher häufig eine zusätzliche Analyse über die Chrome DevTools erforderlich. Wie bei allen automatisierten Prüf-Tools gilt auch bei Lighthouse, dass die Ergebnisse kontextabhängig interpretiert werden müssen und eine manuelle Prüfung für eine vollständige BITV-Bewertung unerlässlich ist.

\section{Fazit automatische Analysetools}

Zunächsteinmal war auffällig, dass alle drei Tools insagesamt nur für sehr wenige Prüfkriterien (zwischen zwei und sechs) Barrieren entdecken konnten. Weiterhin waren die Unterschiede zwischen den Tools auffällig, während Axe und Lighthouse einen eher konservativen Ansatz verfolgten, fand WAVE deutlich mehr Fehler. Es ist somit ratsam mehrere Tools zu verwenden, da unterschiedliche Barrieren auf gleichen Seiten gefunden werden können. Weiterhin war auffällig, dass Barrieren von Tools unterschiedlich Interpretiert werden. Während Axe ein fehlendes Label in den Prüfschritt 9.1.3.1 einordnete, verordnete WAVE den Fehler für das gleiche Eingabelemente unter Prüfschritt 9.3.3.2. Diese unterschiedliche Einordnung von Barrieren macht es für den Tester schwierig Tools zu vergleichen. Es muss immer auf Toolspezifische Muster geachtet werden, bei der Verwendung und speziell beim Vergleich der Ergebnisse solcher Analysetools.